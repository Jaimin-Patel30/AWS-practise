{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ashruti.patel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "C:\\Users\\ashruti.patel\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import math\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import nltk\n",
    "import gensim\n",
    "import csv\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from collections import Counter\n",
    "import math\n",
    "from nltk import word_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "from google.cloud import language\n",
    "from google.cloud.language import enums\n",
    "from google.cloud.language import types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "import re\n",
    "from subprocess import check_output\n",
    "from nltk.metrics import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_sick(f): \n",
    "\n",
    "    response = requests.get(f).text\n",
    "\n",
    "    lines = response.split(\"\\n\")[1:]\n",
    "    lines = [l.split(\"\\t\") for l in lines if len(l) > 0]\n",
    "    lines = [l for l in lines if len(l) == 5]\n",
    "\n",
    "    df = pd.DataFrame(lines, columns=[\"idx\", \"sent_1\", \"sent_2\", \"sim\", \"label\"])\n",
    "    df['sim'] = pd.to_numeric(df['sim'])\n",
    "    return df\n",
    "    \n",
    "sick_train = download_sick(\"https://raw.githubusercontent.com/alvations/stasis/master/SICK-data/SICK_train.txt\")\n",
    "sick_dev = download_sick(\"https://raw.githubusercontent.com/alvations/stasis/master/SICK-data/SICK_trial.txt\")\n",
    "sick_test = download_sick(\"https://raw.githubusercontent.com/alvations/stasis/master/SICK-data/SICK_test_annotated.txt\")\n",
    "sick_all = sick_train.append(sick_test).append(sick_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH_TO_WORD2VEC = os.path.expanduser(\"D:\\\\Backup\\\\nlp-notebooks-master\\\\data\\\\sentence_similarity\\\\GoogleNews-vectors-negative300.bin\")\n",
    "PATH_TO_GLOVE = os.path.expanduser(\"D:\\\\Backup\\\\nlp-notebooks-master\\\\data\\\\sentence_similarity\\\\glove.840B.300d.txt\")\n",
    "\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format(PATH_TO_WORD2VEC, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PATH_TO_FREQUENCIES_FILE = \"D:\\\\Backup\\\\nlp-notebooks-master\\\\data\\\\sentence_similarity\\\\frequencies.tsv\"\n",
    "PATH_TO_DOC_FREQUENCIES_FILE = \"D:\\\\Backup\\\\nlp-notebooks-master\\\\data\\\\sentence_similarity\\\\doc_frequencies.tsv\"\n",
    "\n",
    "def read_tsv(f):\n",
    "    frequencies = {}\n",
    "    with open(f) as tsv:\n",
    "        tsv_reader = csv.reader(tsv, delimiter=\"\\t\")\n",
    "        for row in tsv_reader: \n",
    "            frequencies[row[0]] = int(row[1])\n",
    "        \n",
    "    return frequencies\n",
    "        \n",
    "frequencies = read_tsv(PATH_TO_FREQUENCIES_FILE)\n",
    "doc_frequencies = read_tsv(PATH_TO_DOC_FREQUENCIES_FILE)\n",
    "doc_frequencies[\"NUM_DOCS\"] = 1288431"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#method 1\n",
    "def run_avg_benchmark(sentences1, sentences2, model=None, use_stoplist=False, doc_freqs=None): \n",
    "\n",
    "    if doc_freqs is not None:\n",
    "        N = doc_freqs[\"NUM_DOCS\"]\n",
    "    \n",
    "    sims = []\n",
    "    for (sent1, sent2) in zip(sentences1, sentences2):\n",
    "    \n",
    "        tokens1 = sent1.tokens_without_stop if use_stoplist else sent1\n",
    "        tokens2 = sent2.tokens_without_stop if use_stoplist else sent2\n",
    "\n",
    "        tokens1 = [token for token in tokens1 if token in model]\n",
    "        tokens2 = [token for token in tokens2 if token in model]\n",
    "        \n",
    "        if len(tokens1) == 0 or len(tokens2) == 0:\n",
    "            sims.append(0)\n",
    "            continue\n",
    "        \n",
    "        tokfreqs1 = Counter(tokens1)\n",
    "        tokfreqs2 = Counter(tokens2)\n",
    "        \n",
    "        weights1 = [tokfreqs1[token] * math.log(N/(doc_freqs.get(token, 0)+1)) \n",
    "                    for token in tokfreqs1] if doc_freqs else None\n",
    "        weights2 = [tokfreqs2[token] * math.log(N/(doc_freqs.get(token, 0)+1)) \n",
    "                    for token in tokfreqs2] if doc_freqs else None\n",
    "                \n",
    "        embedding1 = np.average([model[token] for token in tokfreqs1], axis=0, weights=weights1).reshape(1, -1)\n",
    "        embedding2 = np.average([model[token] for token in tokfreqs2], axis=0, weights=weights2).reshape(1, -1)\n",
    "\n",
    "        sim = cosine_similarity(embedding1, embedding2)[0][0]\n",
    "        sims.append(sim)\n",
    "    print (sum(sims) / float(len(sims)))\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_avg_benchmark(\"my name is jimmy\", \"my name is not jaimin\",model=word2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_obama = 'Obama speaks to the media in Illinois'.lower().split()\n",
    "sentence_president = 'The president greets the press in Chicago'.lower().split()\n",
    "word_vectors = api.load(\"glove-wiki-gigaword-100\")\n",
    "\n",
    "#method 2\n",
    "word_vectors.wmdistance(sentence_obama, sentence_president)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]=\"C:/astral-shape-187315-e8e3ba35bd82.json\"\n",
    "\n",
    "#method 3\n",
    "def findSentiment(sentense):\n",
    "    client = language.LanguageServiceClient()\n",
    "\n",
    "    document = types.Document(\n",
    "        content=sentense,\n",
    "        type=enums.Document.Type.PLAIN_TEXT)\n",
    "    jsonStr = client.analyze_sentiment(document=document)\n",
    "\n",
    "    return jsonStr.document_sentiment.score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "findSentiment(\"you are not good\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def tokenize(q1, q2):\n",
    "    \"\"\"\n",
    "        q1 and q2 are sentences/questions. Function returns a list of tokens for both.\n",
    "    \"\"\"\n",
    "    return word_tokenize(q1), word_tokenize(q2)\n",
    "\n",
    "\n",
    "def posTag(q1, q2):\n",
    "    \"\"\"\n",
    "        q1 and q2 are lists. Function returns a list of POS tagged tokens for both.\n",
    "    \"\"\"\n",
    "    return nltk.pos_tag(q1), nltk.pos_tag(q2)\n",
    "\n",
    "\n",
    "def stemmer(tag_q1, tag_q2):\n",
    "    \"\"\"\n",
    "        tag_q = tagged lists. Function returns a stemmed list.\n",
    "    \"\"\"\n",
    "\n",
    "    stem_q1 = []\n",
    "    stem_q2 = []\n",
    "\n",
    "    for token in tag_q1:\n",
    "        stem_q1.append(stem(token))\n",
    "\n",
    "    for token in tag_q2:\n",
    "        stem_q2.append(stem(token))\n",
    "\n",
    "    return stem_q1, stem_q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lesk(object):\n",
    "\n",
    "    def __init__(self, sentence):\n",
    "        self.sentence = sentence\n",
    "        self.meanings = {}\n",
    "        for word in sentence:\n",
    "            self.meanings[word] = ''\n",
    "\n",
    "    def getSenses(self, word):\n",
    "        # print word\n",
    "        return wn.synsets(word.lower())\n",
    "\n",
    "    def getGloss(self, senses):\n",
    "\n",
    "        gloss = {}\n",
    "\n",
    "        for sense in senses:\n",
    "            gloss[sense.name()] = []\n",
    "\n",
    "        for sense in senses:\n",
    "            gloss[sense.name()] += word_tokenize(sense.definition())\n",
    "\n",
    "        return gloss\n",
    "\n",
    "    def getAll(self, word):\n",
    "        senses = self.getSenses(word)\n",
    "\n",
    "        if senses == []:\n",
    "            return {word.lower(): senses}\n",
    "\n",
    "        return self.getGloss(senses)\n",
    "\n",
    "    def Score(self, set1, set2):\n",
    "        # Base\n",
    "        overlap = 0\n",
    "\n",
    "        # Step\n",
    "        for word in set1:\n",
    "            if word in set2:\n",
    "                overlap += 1\n",
    "\n",
    "        return overlap\n",
    "\n",
    "    def overlapScore(self, word1, word2):\n",
    "\n",
    "        gloss_set1 = self.getAll(word1)\n",
    "        if self.meanings[word2] == '':\n",
    "            gloss_set2 = self.getAll(word2)\n",
    "        else:\n",
    "            # print 'here'\n",
    "            gloss_set2 = self.getGloss([wn.synset(self.meanings[word2])])\n",
    "\n",
    "        # print gloss_set2\n",
    "\n",
    "        score = {}\n",
    "        for i in gloss_set1.keys():\n",
    "            score[i] = 0\n",
    "            for j in gloss_set2.keys():\n",
    "                score[i] += self.Score(gloss_set1[i], gloss_set2[j])\n",
    "\n",
    "        bestSense = None\n",
    "        max_score = 0\n",
    "        for i in gloss_set1.keys():\n",
    "            if score[i] > max_score:\n",
    "                max_score = score[i]\n",
    "                bestSense = i\n",
    "\n",
    "        return bestSense, max_score\n",
    "\n",
    "    def lesk(self, word, sentence):\n",
    "        maxOverlap = 0\n",
    "        context = sentence\n",
    "        word_sense = []\n",
    "        meaning = {}\n",
    "\n",
    "        senses = self.getSenses(word)\n",
    "\n",
    "        for sense in senses:\n",
    "            meaning[sense.name()] = 0\n",
    "\n",
    "        for word_context in context:\n",
    "            if not word == word_context:\n",
    "                score = self.overlapScore(word, word_context)\n",
    "                if score[0] == None:\n",
    "                    continue\n",
    "                meaning[score[0]] += score[1]\n",
    "\n",
    "        if senses == []:\n",
    "            return word, None, None\n",
    "\n",
    "        self.meanings[word] = max(meaning.keys(), key=lambda x: meaning[x])\n",
    "\n",
    "        return word, self.meanings[word], wn.synset(self.meanings[word]).definition()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def path(set1, set2):\n",
    "    return wn.path_similarity(set1, set2)\n",
    "\n",
    "\n",
    "def wup(set1, set2):\n",
    "    return wn.wup_similarity(set1, set2)\n",
    "\n",
    "\n",
    "def edit(word1, word2):\n",
    "    if float(edit_distance(word1, word2)) == 0.0:\n",
    "        return 0.0\n",
    "    return 1.0 / float(edit_distance(word1, word2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computePath(q1, q2):\n",
    "\n",
    "    R = np.zeros((len(q1), len(q2)))\n",
    "\n",
    "    for i in range(len(q1)):\n",
    "        for j in range(len(q2)):\n",
    "            if q1[i][1] == None or q2[j][1] == None:\n",
    "                sim = edit(q1[i][0], q2[j][0])\n",
    "            else:\n",
    "                sim = path(wn.synset(q1[i][1]), wn.synset(q2[j][1]))\n",
    "\n",
    "            if sim == None:\n",
    "                sim = edit(q1[i][0], q2[j][0])\n",
    "\n",
    "            R[i, j] = sim\n",
    "\n",
    "    # print R\n",
    "\n",
    "    return R\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def computeWup(q1, q2):\n",
    "\n",
    "    R = np.zeros((len(q1), len(q2)))\n",
    "\n",
    "    for i in range(len(q1)):\n",
    "        for j in range(len(q2)):\n",
    "            if q1[i][1] == None or q2[j][1] == None:\n",
    "                sim = edit(q1[i][0], q2[j][0])\n",
    "            else:\n",
    "                sim = wup(wn.synset(q1[i][1]), wn.synset(q2[j][1]))\n",
    "\n",
    "            if sim == None:\n",
    "                sim = edit(q1[i][0], q2[j][0])\n",
    "\n",
    "            R[i, j] = sim\n",
    "\n",
    "    # print R\n",
    "\n",
    "    return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overallSim(q1, q2, R):\n",
    "\n",
    "    sum_X = 0.0\n",
    "    sum_Y = 0.0\n",
    "\n",
    "    for i in range(len(q1)):\n",
    "        max_i = 0.0\n",
    "        for j in range(len(q2)):\n",
    "            if R[i, j] > max_i:\n",
    "                max_i = R[i, j]\n",
    "        sum_X += max_i\n",
    "\n",
    "    for i in range(len(q1)):\n",
    "        max_j = 0.0\n",
    "        for j in range(len(q2)):\n",
    "            if R[i, j] > max_j:\n",
    "                max_j = R[i, j]\n",
    "        sum_Y += max_j\n",
    "        \n",
    "    if (float(len(q1)) + float(len(q2))) == 0.0:\n",
    "        return 0.0\n",
    "        \n",
    "    overall = (sum_X + sum_Y) / (2 * (float(len(q1)) + float(len(q2))))\n",
    "\n",
    "    return overall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semanticSimilarity(q1, q2):\n",
    "\n",
    "    tokens_q1, tokens_q2 = tokenize(q1, q2)\n",
    "    # stem_q1, stem_q2 = stemmer(tokens_q1, tokens_q2)\n",
    "    tag_q1, tag_q2 = posTag(tokens_q1, tokens_q2)\n",
    "\n",
    "    sentence = []\n",
    "    for i, word in enumerate(tag_q1):\n",
    "        if 'NN' in word[1] or 'JJ' in word[1] or 'VB' in word[1]:\n",
    "            sentence.append(word[0])\n",
    "\n",
    "    sense1 = Lesk(sentence)\n",
    "    sentence1Means = []\n",
    "    for word in sentence:\n",
    "        sentence1Means.append(sense1.lesk(word, sentence))\n",
    "\n",
    "    sentence = []\n",
    "    for i, word in enumerate(tag_q2):\n",
    "        if 'NN' in word[1] or 'JJ' in word[1] or 'VB' in word[1]:\n",
    "            sentence.append(word[0])\n",
    "\n",
    "    sense2 = Lesk(sentence)\n",
    "    sentence2Means = []\n",
    "    for word in sentence:\n",
    "        sentence2Means.append(sense2.lesk(word, sentence))\n",
    "    # for i, word in enumerate(sentence1Means):\n",
    "    #     print sentence1Means[i][0], sentence2Means[i][0]\n",
    "\n",
    "    R1 = computePath(sentence1Means, sentence2Means)\n",
    "    R2 = computeWup(sentence1Means, sentence2Means)\n",
    "\n",
    "    R = (R1 + R2) / 2\n",
    "\n",
    "    # print R\n",
    "\n",
    "    return overallSim(sentence1Means, sentence2Means, R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ashruti.patel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "STOP_WORDS = nltk.download('stopwords')\n",
    "def clean_sentence(val):\n",
    "    \"remove chars that are not letters or numbers, downcase, then remove stop words\"\n",
    "    regex = re.compile('([^\\s\\w]|_)+')\n",
    "    sentence = regex.sub('', val).lower()\n",
    "    sentence = sentence.split(\" \")\n",
    "\n",
    "    for word in list(sentence):\n",
    "        if word in STOP_WORDS:\n",
    "            sentence.remove(word)\n",
    "\n",
    "    sentence = \" \".join(sentence)\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ashruti.patel\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "STOP_WORDS = nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semanticSimilarity(\"obama is the president of USA\", \"USA has a president named Obama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
